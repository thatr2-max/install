# AI Mod Generator - Agentic Architecture TODO

## Project Overview
An AI agent that creates game mods by intelligently using tools to research, plan, generate, validate, and refine. The agent learns modding patterns from examples and documentation, then uses an agentic workflow with tools to create high-quality mods. Uses OpenRouter API with Qwen3-Next-80B-A3B-Instruct.

## Philosophy
This isn't just a prompt-and-pray generator. This is a REAL AI AGENT that:
- Researches before generating (reads examples, docs, schemas)
- Plans based on self-assessed complexity
- Validates its own work
- Iterates until the mod is actually good
- Uses tools like a human modder would

## Core Architecture

### Directory Structure
```
ai-mod-generator/
├── game_schemas/          # Learned patterns for each game
│   ├── cdda/
│   │   ├── schema.json           # Extracted structure/patterns
│   │   ├── examples/             # User-provided example mods
│   │   ├── docs/                 # User-provided modding documentation
│   │   └── vanilla_data/         # Extracted vanilla game values for balance comparison
│   ├── rimworld/
│   └── factorio/
├── output/                # Generated mods (ready to copy to game)
├── plans/                 # Saved planning phases for debugging
├── logs/                  # Agent decision logs and tool usage
├── src/
│   ├── agent/
│   │   ├── modding_agent.py      # Main agentic orchestrator
│   │   ├── tools.py              # Tool definitions and implementations
│   │   ├── system_prompt.py      # Dynamic system prompt builder
│   │   └── tool_executor.py      # Tool execution engine
│   ├── learner/
│   │   ├── schema_learner.py     # Schema extraction from examples/docs
│   │   ├── pattern_extractor.py  # Identify common patterns
│   │   └── doc_processor.py      # Documentation parsing
│   ├── validation/
│   │   ├── syntax_validator.py   # JSON/XML/Lua syntax checking
│   │   ├── reference_checker.py  # Verify ID references exist
│   │   ├── balance_analyzer.py   # Compare to vanilla values
│   │   └── schema_validator.py   # Match learned patterns
│   ├── openrouter_client.py      # OpenRouter API wrapper
│   └── utils/
│       ├── file_utils.py
│       └── token_counter.py
├── cli.py                # Main CLI interface
├── config.json           # OpenRouter API key, model selection
└── requirements.txt
```

## Phase 1: Foundation Setup

### 1.1 Project Initialization
- [ ] Create Python project structure as outlined above
- [ ] Set up virtual environment
- [ ] Create requirements.txt with dependencies:
  - `openai` (for OpenRouter API compatibility - they use OpenAI format)
  - `click` (CLI framework)
  - `pyyaml` (config parsing)
  - `jsonschema` (validation)
  - `rich` (pretty CLI output with progress bars)
  - `tiktoken` (token counting for cost tracking)
  - `pypdf2` (PDF documentation extraction)
  - `lxml` (XML parsing for RimWorld/Factorio)
  - `python-Levenshtein` (fuzzy matching for reference checking)

### 1.2 Configuration System
- [ ] Create config.json structure:
  ```json
  {
    "openrouter_api_key": "PLACEHOLDER",
    "model": "qwen/qwen3-next-80b-a3b-instruct",
    "base_url": "https://openrouter.ai/api/v1",
    "max_iterations": 10,
    "enable_tool_use": true,
    "log_level": "INFO",
    "supported_games": [
      "cdda",
      "rimworld", 
      "factorio",
      "project_zomboid",
      "terraria",
      "stardew_valley"
    ],
    "cost_tracking": {
      "input_cost_per_1m_tokens": 0.15,
      "output_cost_per_1m_tokens": 1.50,
      "warn_threshold_usd": 1.00
    }
  }
  ```
- [ ] Create config loader in src/config.py
- [ ] Add API key validation on startup
- [ ] Add budget tracking and warnings

### 1.3 OpenRouter Client with Tool Support
- [ ] Implement OpenRouterClient class in src/openrouter_client.py
  - [ ] Initialize with API key from config
  - [ ] Implement chat completion method with tool support (OpenAI format)
  - [ ] Add token counting for cost tracking
  - [ ] Include retry logic with exponential backoff
  - [ ] Add streaming support for large generations
  - [ ] Support tool call parsing from response
  - [ ] Handle function_call and tool_calls response types
  - [ ] Log all API calls with token usage to logs/api_usage.log
  - [ ] Track cumulative costs in logs/cost_tracker.json

## Phase 2: Schema Learning System

### 2.1 Example Analysis
- [ ] Implement analyze_examples() in src/learner/schema_learner.py
  - [ ] Recursively scan game's examples/ folder for all mod files
  - [ ] Detect file types (JSON, XML, Lua, etc.)
  - [ ] Extract structural patterns:
    - Required fields vs optional fields
    - Data types (string, int, array, nested objects)
    - Common value ranges/enums
    - Naming conventions (snake_case, PascalCase, etc.)
    - File organization patterns (flat vs nested)
  - [ ] Identify common templates (items, recipes, entities, etc.)
  - [ ] Build frequency map of patterns (what fields appear together)
  - [ ] Extract actual values for balance reference

### 2.2 Documentation Processing  
- [ ] Implement process_docs() in src/learner/doc_processor.py
  - [ ] Read all files in game's docs/ folder
  - [ ] Support multiple formats: .md, .txt, .pdf (extract text)
  - [ ] Extract modding rules, field descriptions, constraints
  - [ ] Build knowledge base of game-specific mechanics
  - [ ] Create searchable index of documentation
  - [ ] Associate documentation with structural patterns from examples
  - [ ] Build glossary of game-specific terms

### 2.3 Vanilla Data Extraction
- [ ] Implement extract_vanilla_data() in src/learner/pattern_extractor.py
  - [ ] Parse vanilla game files from examples to build baseline
  - [ ] Extract value ranges for balance checking:
    - Damage ranges by weapon type
    - Cost ranges by item category  
    - Spawn rates
    - Resource consumption rates
    - Skill requirements
  - [ ] Build statistical profiles (mean, median, stddev) for numeric values
  - [ ] Save to game_schemas/[game]/vanilla_data/stats.json

### 2.4 Schema Generation
- [ ] Implement generate_schema() in src/learner/schema_learner.py
  - [ ] Combine insights from examples and docs
  - [ ] Create schema.json with:
    ```json
    {
      "game": "cdda",
      "version": "1.0",
      "learned_at": "2025-11-09T12:00:00Z",
      "example_count": 15,
      "file_types": ["json"],
      "mod_structure": {
        "required_files": ["modinfo.json"],
        "optional_files": ["items/*.json", "recipes/*.json"],
        "naming_conventions": {
          "items": "snake_case",
          "ids": "lowercase_underscore"
        }
      },
      "templates": {
        "item": {
          "pattern": {...},
          "required_fields": ["id", "type", "name"],
          "optional_fields": ["description", "weight", "volume"],
          "field_descriptions": {...},
          "common_values": {
            "type": ["GENERIC", "ARMOR", "WEAPON"],
            "material": ["steel", "wood", "plastic"]
          }
        },
        "recipe": {...},
        "profession": {...}
      },
      "mechanics_knowledge": {
        "damage_types": ["bash", "cut", "pierce", "fire"],
        "skill_names": ["mechanics", "fabrication", "cooking"],
        "balance_guidelines": "Damage typically ranges 10-100 for handheld weapons"
      },
      "vanilla_baselines": {
        "weapon_damage": {"mean": 45.2, "median": 40, "stddev": 15.3},
        "item_weight": {"mean": 500, "median": 350, "stddev": 200}
      }
    }
    ```
  - [ ] Save to game_schemas/[game]/schema.json
  - [ ] Include version/timestamp for schema updates
  - [ ] Create schema_summary.txt for quick reference

## Phase 3: Tools System (The Agent's Superpowers)

### 3.1 Tool Registry and Executor
- [ ] Implement ToolExecutor class in src/agent/tool_executor.py
  - [ ] Register all available tools with signatures
  - [ ] Convert tool definitions to OpenAI function format
  - [ ] Execute tool calls from AI responses
  - [ ] Log all tool usage to logs/tool_usage.log
  - [ ] Handle tool errors gracefully
  - [ ] Return results in format AI can understand

### 3.2 Schema Research Tools
- [ ] Implement in src/agent/tools.py:

**view_schema_file(game: str, section: str) -> dict**
- Read specific section of learned schema (e.g., "item_template", "balance_guidelines")
- Returns focused schema snippet to avoid token bloat

**list_template_types(game: str) -> list**
- List all available templates (items, recipes, etc.)
- Returns template names with brief descriptions

**get_template(game: str, template_type: str) -> dict**
- Get full template with all fields and examples
- Returns complete template structure

### 3.3 Example Analysis Tools
- [ ] Implement in src/agent/tools.py:

**list_examples(game: str, filter_type: str = None) -> list**
- Browse available example mods
- Optional filter by mod type (weapons, items, etc.)
- Returns list with mod names and brief descriptions

**read_example_mod(game: str, example_name: str) -> dict**
- Deep read a specific example mod file
- Returns full file contents with annotations
- Includes file structure and key patterns

**analyze_similar_mods(game: str, description: str) -> list**
- Find example mods similar to user's request
- Uses embedding similarity or keyword matching
- Returns 2-3 most relevant examples with explanations

**extract_pattern(game: str, pattern_type: str, examples: list) -> dict**
- Analyze multiple examples to extract common pattern
- E.g., "how do all weapon mods structure damage?"
- Returns generalized pattern with variations

### 3.4 Documentation Tools
- [ ] Implement in src/agent/tools.py:

**search_documentation(game: str, query: str) -> str**
- Search game's modding documentation
- Returns relevant snippets with context
- Includes source file references

**get_field_description(game: str, field_path: str) -> str**
- Get documentation for specific field (e.g., "item.damage.pierce")
- Returns field meaning, type, constraints, examples

### 3.5 Validation Tools
- [ ] Implement in src/validation/:

**validate_json(content: str) -> dict**
- Check if JSON is syntactically valid
- Returns: {"valid": bool, "errors": [...]}

**validate_xml(content: str) -> dict**
- Check if XML is well-formed
- Returns: {"valid": bool, "errors": [...]}

**check_references(game: str, mod_files: dict) -> dict**
- Verify all IDs referenced actually exist
- Cross-check between files
- Returns: {"valid": bool, "missing_refs": [...], "warnings": [...]}

**balance_check(game: str, item_stats: dict, item_type: str) -> dict**
- Compare stats to vanilla baseline
- Flag outliers (>2 std devs from mean)
- Returns: {"balanced": bool, "warnings": [...], "suggestions": [...]}

**schema_compliance(game: str, mod_files: dict) -> dict**
- Verify structure matches learned schema
- Check required fields, data types, naming conventions
- Returns: {"compliant": bool, "violations": [...]}

### 3.6 Iterative Refinement Tools
- [ ] Implement in src/agent/tools.py:

**save_draft(game: str, draft_name: str, content: dict) -> str**
- Save work-in-progress mod for iteration
- Returns: draft path

**load_draft(game: str, draft_name: str) -> dict**
- Load previously saved draft
- Returns: draft content

**compare_to_vanilla(game: str, item_name: str, proposed_stats: dict) -> dict**
- Compare proposed item to similar vanilla items
- Returns comparison with balance assessment

### 3.7 Tool Definitions Export
- [ ] Create tool_definitions.json with all tool signatures in OpenAI format:
  ```json
  [
    {
      "type": "function",
      "function": {
        "name": "view_schema_file",
        "description": "Read a specific section of the learned game schema",
        "parameters": {
          "type": "object",
          "properties": {
            "game": {"type": "string", "description": "Game identifier"},
            "section": {"type": "string", "description": "Schema section to read"}
          },
          "required": ["game", "section"]
        }
      }
    },
    ...
  ]
  ```

## Phase 4: CLI Interface

### 4.1 Learning Commands
- [ ] Implement `modgen learn [game]` command
  - [ ] Prompt user to place example mods in game_schemas/[game]/examples/
  - [ ] Prompt user to place docs in game_schemas/[game]/docs/
  - [ ] Run schema learning pipeline with progress display
  - [ ] Extract vanilla data for balance checking
  - [ ] Save schema and confirm completion
  - [ ] Display: example count, templates found, token usage, estimated API cost

### 4.2 Generation Commands  
- [ ] Implement `modgen create [game] "[description]"` command
  - [ ] Validate game has learned schema
  - [ ] Accept natural language mod description
  - [ ] Optional flags:
    - `--output-dir` (custom output location)
    - `--verbose` (show agent reasoning, tool calls, planning)
    - `--no-validate` (skip validation phase - not recommended)
    - `--max-iterations` (override default iteration limit)
    - `--save-plan` (save planning phase to plans/ directory)
  - [ ] Run agentic generation workflow
  - [ ] Display real-time progress:
    - Planning phase progress
    - Tool calls being made
    - Validation results
    - Refinement iterations
  - [ ] Save to output/[game]/[mod_name]_[timestamp]/
  - [ ] Display file tree of generated mod
  - [ ] Show token usage, cost, and tool call count
  - [ ] Display any warnings or compatibility notes

### 4.3 Utility Commands
- [ ] Implement `modgen list` - show all learned games with details
- [ ] Implement `modgen info [game]` - show schema details, example count, templates available
- [ ] Implement `modgen validate [game] [mod_path]` - validate existing mod files
- [ ] Implement `modgen relearn [game]` - rebuild schema from examples/docs
- [ ] Implement `modgen cost` - show total API usage and cost so far
- [ ] Implement `modgen plans` - list all saved planning phases
- [ ] Implement `modgen tools` - list all available tools and their descriptions

### 4.4 Advanced Commands
- [ ] Implement `modgen refine [game] [mod_path] "[instructions]"` 
  - Improve existing mod based on feedback
  - Agent loads mod, reads instructions, uses tools to refine
- [ ] Implement `modgen batch [game] [csv_file]`
  - Generate multiple mods from CSV with descriptions
  - Track progress and costs
- [ ] Implement `modgen debug [game] --plan-file [path]`
  - Replay a planning phase to debug issues
  - Show all tool calls and decisions

## Phase 5: Agentic Generation Engine

### 5.1 System Prompt Builder
- [ ] Implement build_system_prompt() in src/agent/system_prompt.py
  - [ ] Create dynamic system prompt based on game and available tools
  - [ ] Include:
    ```python
    SYSTEM_PROMPT_TEMPLATE = """
You are an expert {game} modder with access to powerful tools that make you exceptional at your craft.

AVAILABLE TOOLS:
{tool_list_with_descriptions}

YOUR AGENTIC WORKFLOW FOR CREATING MODS:

1. UNDERSTAND THE REQUEST
   - Parse what the user wants
   - Identify key requirements and constraints

2. RESEARCH PHASE (Use tools liberally)
   - search_documentation: Find relevant modding info about mechanics mentioned
   - analyze_similar_mods: See how others solved similar problems
   - view_schema_file: Understand required data structures  
   - read_example_mod: Study best practices from examples
   - get_template: Get templates for content types you need

3. SELF-ASSESSMENT & PLANNING (ALWAYS DO THIS)
   - Assess complexity: Is this simple (value tweak), medium (new content using templates), or complex (new systems)?
   - Write your plan based on complexity:
     * SIMPLE: 3-5 line plan with key steps
     * MEDIUM: Structured plan covering files, balance, dependencies
     * COMPLEX: Detailed plan with edge cases, interactions, testing
   - Think: "How long will this take to implement correctly?" Plan accordingly
   - Save plan using save_draft with name "plan"

4. GENERATION PHASE
   - Create mod files following your plan
   - Use patterns from your research
   - Maintain consistent naming and structure
   - Reference your plan if you get stuck

5. VALIDATION PHASE (CRITICAL - Always validate)
   - validate_json/validate_xml: Check syntax
   - check_references: Ensure all IDs exist
   - balance_check: Flag extreme values vs vanilla
   - schema_compliance: Verify structure matches patterns
   
6. REFINEMENT PHASE (If validation fails or you see issues)
   - Analyze validation errors
   - Use tools to understand what went wrong
   - Fix issues systematically
   - Re-validate until clean

7. FINALIZATION
   - Output all files with delimiters: --- FILE: path/to/file.ext ---
   - Include installation instructions in README.txt
   - Note warnings or compatibility concerns

CRITICAL RULES:
- ALWAYS research before generating (especially for complex mods)
- ALWAYS create a plan (no exceptions, even for simple mods)
- ALWAYS validate before finalizing
- ALWAYS check references between files  
- If stuck, use analyze_similar_mods or read more examples
- If unsure about balance, use balance_check and compare_to_vanilla
- Tool calls are cheap compared to regenerating broken mods - use them liberally
- Iterate until the mod is actually good, not just "good enough"

PHILOSOPHY:
You're not a text generator - you're an AI modder. Think like a human expert would:
research unfamiliar concepts, plan before coding, validate your work, and iterate 
until excellent. Your tools are your superpowers - use them.

Remember: Quality > Speed. A working mod after 8 tool calls beats a broken mod in 1.
"""
    ```
  - [ ] Inject game-specific mechanics knowledge
  - [ ] Include schema summary (not full schema - that comes via tools)
  - [ ] Add game-specific warnings (e.g., "CDDA requires unique IDs")

### 5.2 Agentic Orchestration
- [ ] Implement ModdingAgent class in src/agent/modding_agent.py
  - [ ] Initialize with OpenRouterClient, game schemas, tools
  - [ ] Build tool registry from tool implementations
  - [ ] Main method: `generate_mod(game, description, max_iterations=10)`
  
  ```python
  def generate_mod(self, game: str, description: str, max_iterations: int = 10):
      """
      Run agentic workflow to create mod
      
      Returns: {
          "success": bool,
          "output_files": dict,
          "plan": str,
          "iterations": int,
          "tool_calls": list,
          "cost": float,
          "warnings": list
      }
      """
      # Build context
      system_prompt = self.build_system_prompt(game)
      messages = [
          {"role": "system", "content": system_prompt},
          {"role": "user", "content": f"Create this mod: {description}"}
      ]
      
      iteration = 0
      tool_call_log = []
      
      while iteration < max_iterations:
          # Call AI with tools
          response = self.client.chat(
              messages=messages,
              tools=self.get_tool_definitions(),
              model="qwen/qwen3-next-80b-a3b-instruct"
          )
          
          # AI wants to use tools
          if response.has_tool_calls():
              for tool_call in response.tool_calls:
                  # Execute tool
                  result = self.execute_tool(tool_call, game)
                  tool_call_log.append({
                      "tool": tool_call.name,
                      "args": tool_call.arguments,
                      "result": result
                  })
                  
                  # Add tool result to messages
                  messages.append({
                      "role": "assistant",
                      "content": None,
                      "tool_calls": [tool_call.to_dict()]
                  })
                  messages.append({
                      "role": "tool",
                      "tool_call_id": tool_call.id,
                      "content": json.dumps(result)
                  })
              
              iteration += 1
              continue
          
          # AI has final answer
          if response.is_final():
              return self.parse_final_output(
                  response.content,
                  tool_call_log,
                  game
              )
          
          iteration += 1
      
      raise Exception(f"Max iterations ({max_iterations}) exceeded")
  ```

### 5.3 Tool Execution Logic
- [ ] Implement execute_tool() method
  - [ ] Route tool name to implementation
  - [ ] Pass game context when needed
  - [ ] Handle tool errors gracefully
  - [ ] Log execution to logs/tool_usage.log
  - [ ] Return result in consistent format:
    ```python
    {
        "success": bool,
        "data": any,
        "error": str | None,
        "metadata": dict  # e.g., source files, confidence scores
    }
    ```

### 5.4 Response Parsing
- [ ] Implement parse_final_output() in src/agent/modding_agent.py
  - [ ] Extract plan from tool calls (draft named "plan")
  - [ ] Parse file delimiters from AI response
  - [ ] Support multiple delimiter formats:
    ```
    --- FILE: path/to/file.json ---
    {content}
    
    --- FILE: another/file.xml ---
    <content>
    ```
  - [ ] Detect file type from extension
  - [ ] Clean formatting (fix indentation, remove markdown artifacts)
  - [ ] Validate basic syntax before returning
  - [ ] Extract README/installation instructions
  - [ ] Organize into output structure:
    ```python
    {
        "mod_name": str,
        "files": {
            "path/to/file.json": "content",
            "README.txt": "content"
        },
        "plan": str,
        "metadata": {
            "created_at": datetime,
            "description": str,
            "game": str,
            "tool_calls_used": int,
            "iterations": int,
            "validation_passed": bool
        }
    }
    ```

### 5.5 Self-Assessed Planning Integration
- [ ] Agent automatically initiates planning via save_draft tool
- [ ] No manual "planning phase" - AI decides when to plan
- [ ] Complexity assessment happens naturally in AI's thinking
- [ ] Plan saved to plans/[game]_[sanitized_description]_[timestamp].txt
- [ ] Plan included in final output metadata

## Phase 6: Validation System

### 6.1 Syntax Validation
- [ ] Implement SyntaxValidator in src/validation/syntax_validator.py
  - [ ] JSON validation using jsonschema
    - Parse and validate JSON structure
    - Check for duplicate keys
    - Verify proper escaping
  - [ ] XML validation using lxml
    - Check well-formedness
    - Validate against DTD if available
    - Check namespace correctness
  - [ ] Lua validation (basic)
    - Check for syntax errors using luac or similar
    - Validate function calls and variable names
  - [ ] Return detailed error messages with line numbers

### 6.2 Reference Checking
- [ ] Implement ReferenceChecker in src/validation/reference_checker.py
  - [ ] Build ID registry from all files in mod
  - [ ] Check cross-file references
  - [ ] Verify IDs are unique within scope
  - [ ] Check references to vanilla game content (if schema includes vanilla IDs)
  - [ ] Fuzzy matching for typos ("steel_sword" vs "steel_sward")
  - [ ] Return:
    ```python
    {
        "valid": bool,
        "missing_references": [
            {"file": str, "line": int, "id": str, "type": str}
        ],
        "duplicate_ids": [...],
        "suggestions": [  # Fuzzy matches for missing refs
            {"missing": str, "did_you_mean": str, "confidence": float}
        ]
    }
    ```

### 6.3 Balance Analysis
- [ ] Implement BalanceAnalyzer in src/validation/balance_analyzer.py
  - [ ] Load vanilla baseline stats for game
  - [ ] Compare proposed values to baselines
  - [ ] Statistical analysis:
    - Calculate z-scores for numeric values
    - Flag outliers (>2 std deviations)
    - Identify suspicious combinations (high damage + low cost)
  - [ ] Game-specific balance checks:
    - CDDA: Volume/weight consistency, ammo consumption rates
    - RimWorld: Work amounts vs material costs
    - Factorio: Recipe ratios and production chains
  - [ ] Return:
    ```python
    {
        "balanced": bool,
        "warnings": [
            {
                "field": str,
                "value": any,
                "vanilla_mean": float,
                "z_score": float,
                "severity": "low" | "medium" | "high",
                "suggestion": str
            }
        ],
        "comparisons": {
            "similar_items": [
                {"name": str, "stats": dict, "notes": str}
            ]
        }
    }
    ```

### 6.4 Schema Compliance
- [ ] Implement SchemaValidator in src/validation/schema_validator.py
  - [ ] Load learned schema patterns
  - [ ] Verify structure matches templates
  - [ ] Check required fields are present
  - [ ] Validate data types match expectations
  - [ ] Check naming conventions (snake_case vs PascalCase)
  - [ ] Verify file organization matches learned structure
  - [ ] Validate field value constraints (enums, ranges)
  - [ ] Return:
    ```python
    {
        "compliant": bool,
        "violations": [
            {
                "file": str,
                "rule": str,
                "expected": str,
                "actual": str,
                "severity": "error" | "warning",
                "fix_suggestion": str
            }
        ]
    }
    ```

### 6.5 Validation Orchestrator
- [ ] Implement ValidationOrchestrator in src/validation/orchestrator.py
  - [ ] Run all validators in sequence
  - [ ] Aggregate results
  - [ ] Prioritize errors by severity
  - [ ] Generate validation report
  - [ ] Format results for AI consumption (for refinement phase)
  - [ ] Format results for human display (CLI output)

## Phase 7: Initial Game Support

### 7.1 Per-Game Setup Instructions  
For each game, create a comprehensive README in game_schemas/[game]/README.md with:
- [ ] Where to find good example mods (official forums, Steam Workshop, etc.)
- [ ] Where to get official modding documentation
- [ ] Recommended number and types of examples (5-10, covering different mod types)
- [ ] Common gotchas for that game's modding system
- [ ] File structure requirements
- [ ] Special validation rules
- [ ] Vanilla data sources for balance comparison

### 7.2 Game-Specific Tool Implementations
Each game may need specialized tools:
- [ ] CDDA: Tool for parsing JSON with embedded comments
- [ ] RimWorld: XML namespace handler
- [ ] Factorio: Lua dependency analyzer
- [ ] Project Zomboid: Script file validator
- [ ] Terraria: .tmod format handler (if accessible)
- [ ] Stardew Valley: Content Pack validator

### 7.3 Testing with Real Examples
- [ ] CDDA: Test with items, recipes, professions, monsters
  - Simple: Add weapon variant
  - Medium: Add profession with starting gear
  - Complex: Add new crafting system
- [ ] RimWorld: Test with XML-based mods (items, factions, incidents)
  - Simple: New item with stats
  - Medium: New faction with behavior
  - Complex: New gameplay system with AI
- [ ] Factorio: Test with Lua-based mods (entities, recipes, technologies)
  - Simple: Recipe modification
  - Medium: New machine with recipe
  - Complex: New resource chain
- [ ] Project Zomboid: Test with Lua mods (items, recipes, skills)
- [ ] Terraria: Test with JSON content packs
- [ ] Stardew Valley: Test with Content Patcher format

### 7.4 Agentic Workflow Testing
For each game, verify the agent:
- [ ] Successfully researches before generating
- [ ] Creates appropriate plans for different complexity levels
- [ ] Uses validation tools effectively
- [ ] Iterates when validation fails
- [ ] Learns from examples appropriately
- [ ] Produces working, installable mods

## Phase 8: Polish & Production Readiness

### 8.1 Error Handling
- [ ] Comprehensive try/catch around all API calls
- [ ] User-friendly error messages (no stack traces to users)
- [ ] Graceful degradation if tools fail
- [ ] Timeout handling for long-running operations
- [ ] Rate limit handling with backoff
- [ ] API key validation with clear messages
- [ ] Schema missing/corrupt detection

### 8.2 Cost Management & Transparency
- [ ] Track cumulative token usage across all calls
- [ ] Real-time cost estimates during generation
- [ ] Warn user before expensive operations (>$1.00)
- [ ] Add `--dry-run` flag to estimate cost without executing
- [ ] Cost breakdown by phase (planning, generation, validation)
- [ ] Tool call cost tracking
- [ ] Budget alerts at 50%, 75%, 90% of user-defined limit
- [ ] Monthly/total usage reports

### 8.3 Performance Optimization
- [ ] Cache learned schemas to avoid re-parsing
- [ ] Lazy-load documentation (only load when searched)
- [ ] Stream AI responses for better UX
- [ ] Parallel validation when possible
- [ ] Efficient tool result serialization
- [ ] Minimize context size (only send relevant schema parts)

### 8.4 Logging & Debugging
- [ ] Structured logging to logs/ directory:
  - api_usage.log - All API calls with tokens/cost
  - tool_usage.log - All tool executions
  - agent_decisions.log - High-level workflow decisions
  - errors.log - All errors and exceptions
- [ ] Add `--debug` flag for verbose output
- [ ] Save full conversation history for failed generations
- [ ] Replay capability for debugging

### 8.5 Documentation
- [ ] Write comprehensive README.md:
  - Installation instructions (pip install, API key setup)
  - Quick start guide with example commands
  - Workflow explanation (how the agent works)
  - Game-by-game setup guides
  - Tool reference documentation
  - Troubleshooting section
  - FAQ about costs, iterations, failures
- [ ] Add inline code comments and docstrings
- [ ] Create CONTRIBUTING.md for extending with new games/tools
- [ ] Write example mod descriptions that work well for each game

### 8.6 Output Quality & User Experience
- [ ] Rich CLI formatting (colors, progress bars, tables)
- [ ] Real-time streaming of agent thoughts (in --verbose mode)
- [ ] Clear section headers for workflow phases
- [ ] Pretty-print JSON/XML output files
- [ ] Include metadata in output:
  - Generation timestamp
  - Original description
  - Model used
  - Tool calls made
  - Validation results
  - Installation instructions
- [ ] Auto-generate README.txt with:
  - What the mod does
  - How to install
  - Compatibility notes
  - Known issues/warnings
  - Credits (generated by AI)

### 8.7 Safety & Validation
- [ ] Sanitize user inputs (prevent injection attacks)
- [ ] Validate all file paths (prevent directory traversal)
- [ ] Limit output file size (prevent disk filling)
- [ ] Timeout long-running generations
- [ ] Validate tool call arguments
- [ ] Sandbox tool execution where possible

## Phase 9: Advanced Features (Post-MVP)

### 9.1 Interactive Mode
- [ ] `modgen interactive [game]`
  - Agent asks clarifying questions before generating
  - User can provide feedback during generation
  - Live refinement based on user input

### 9.2 Iterative Refinement
- [ ] `modgen refine [path] "[feedback]"`
  - Load existing mod
  - Apply user feedback using agentic workflow
  - Preserve working parts, fix specific issues

### 9.3 Mod Analysis & Documentation
- [ ] `modgen analyze [path]`
  - Reverse-engineer existing mod
  - Generate documentation
  - Identify balance issues
  - Suggest improvements

### 9.4 Batch Processing
- [ ] CSV-based batch generation
- [ ] Template-based generation (variants of same mod)
- [ ] Parallel processing for multiple mods

### 9.5 Community Features
- [ ] Share learned schemas (community schema repository)
- [ ] Export/import tool configurations
- [ ] Mod quality ratings and feedback
- [ ] Integration with mod hosting platforms

### 9.6 Advanced AI Features
- [ ] Multi-agent collaboration (specialist agents per game)
- [ ] Few-shot learning from user's preferred style
- [ ] Automatic A/B testing of generated variants
- [ ] Explanation generation (why the agent made choices)

### Self-Assessed Planning System
**NO COMPLEXITY TIERS** - The AI self-assesses based on the request.

The agent's prompt includes:
```
"Think: How long will this take to implement correctly? Plan accordingly."
```

The agent naturally thinks:
- "This is just changing a spawn rate → quick 3-line plan"
- "This needs a new weapon category with custom mechanics → detailed 1000-token plan"

Plans are saved via tool: `save_draft("plan", plan_text)`

Then execution phase reads: `load_draft("plan")`

**Why this works:** The AI's reasoning capability is smart enough to gauge complexity without explicit rules. It's like asking a human developer "how complex is this?" - they know intuitively.

### Tool Usage Patterns

**Research Phase:**
```python
# Agent decides to research first
analyze_similar_mods("cdda", "flamethrower weapon")
→ Finds 3 relevant examples

read_example_mod("cdda", "laser_rifle.json")  
→ Studies fuel-based weapon pattern

search_documentation("cdda", "custom ammo types")
→ Gets official docs on ammo system
```

**Validation Phase:**
```python
# Agent validates its work
validate_json(generated_weapon)
→ ✓ Valid

check_references("cdda", all_files)
→ ✗ "cooking_oil_fuel" ID not found

# Agent fixes and re-validates
check_references("cdda", all_files_v2)
→ ✓ All references valid

balance_check("cdda", weapon_stats)
→ ⚠ Damage 150% higher than vanilla, suggests adjustment
```

### OpenRouter Configuration
- **Model**: `qwen/qwen3-next-80b-a3b-instruct`
- **Why this model:**
  - 80B params, only 3B active (cheap but smart)
  - Instruct variant (no `<think>` blocks cluttering output)
  - Native tool calling support
  - Excellent at following complex multi-step workflows
  - Approaches GPT-4o performance at fraction of cost
- **Cost**: $0.15 per 1M input tokens, $1.50 per 1M output tokens
- **Context**: 262K native, we'll use ~32K average for mod generation
- **Temperature**: 0.7 (creative but consistent)
- **Max tokens**: Dynamic based on complexity (1000-8000)

### Token Economics
**Budget $250 breakdown:**

Typical complex mod generation:
- System prompt: ~2000 tokens
- Schema context (via tools): ~1500 tokens
- User description: ~100 tokens
- Planning phase: ~500 input, ~1500 output
- Tool calls (5-10): ~3000 tokens total
- Generation phase: ~3000 input, ~4000 output  
- Validation + refinement: ~2000 tokens

**Total per complex mod:** ~17,000 tokens (~10K input, ~7K output)
**Cost per complex mod:** ~$0.012

**$250 budget:** ~20,000+ complex mods (theoretically)
**Realistic with failures/iterations:** 100-200 excellent mods

Simple mods use ~3000 tokens total (~$0.001 each).

### Prompt Structure Example
```
SYSTEM: [2000 tokens]
You are an expert cdda modder with tools...
[Full workflow instructions]
[Tool descriptions]

USER: Create a flamethrower that uses cooking oil

ASSISTANT: Let me research this systematically.
[calls analyze_similar_mods]

FUNCTION RESULT: [Found 3 examples...]

ASSISTANT: I'll study the laser rifle example
[calls read_example_mod]

FUNCTION RESULT: [laser_rifle.json contents]

ASSISTANT: Now I need ammo documentation
[calls search_documentation]

FUNCTION RESULT: [ammo system docs]

ASSISTANT: [Complexity assessment + planning]
This is MEDIUM complexity. Creating plan:
[calls save_draft with plan]

ASSISTANT: Executing plan...
[generates files]

ASSISTANT: Validating my work
[calls validate_json, check_references, balance_check]

FUNCTION RESULT: [validation results]

ASSISTANT: All validation passed. Here are your files:
--- FILE: items/flamethrower.json ---
{...}
```

### Context Management
The agent maintains full conversation history including:
- All user messages
- All assistant responses
- All tool calls with arguments
- All tool results

This creates a coherent "thread of thought" where the agent remembers what it researched and why.

Max context: 32K tokens (plenty for most mods)
If approaching limit: Summarize older tool results

### File Naming & Organization
**Input:**
```
game_schemas/
  cdda/
    examples/
      laser_rifle/
        items.json
        recipes.json
      molotov/
        items.json
    docs/
      modding_guide.pdf
      json_reference.md
```

**Output:**
```
output/
  cdda/
    flamethrower_weapon_20251109_143022/
      modinfo.json
      items/
        flamethrower.json
        cooking_oil_canister.json
      recipes/
        flamethrower_recipe.json
        oil_canister_recipe.json
      README.txt
      metadata.json

plans/
  cdda_flamethrower_weapon_20251109_143022.txt

logs/
  api_usage.log
  tool_usage.log
  agent_decisions.log
```

### Validation Strategy
**Multi-layer validation:**
1. Syntax (must pass)
2. References (must pass)  
3. Schema compliance (should pass, warnings ok)
4. Balance (warnings only, doesn't block)

**Agent behavior:**
- If syntax fails → Fix immediately and retry
- If references fail → Add missing definitions
- If balance warnings → Adjust if severe, note if minor
- Max 3 refinement iterations before asking user for guidance

### Error Recovery
**If agent gets stuck:**
- Iteration 1-3: Try different approaches
- Iteration 4-6: Use more research tools
- Iteration 7-9: Simplify scope, focus on working subset
- Iteration 10: Return partial result with explanation

**Common failure modes:**
- "I couldn't find documentation on X" → Provide docs or examples
- "Validation keeps failing on Y" → Check schema learning quality
- "Cost exceeding expectations" → Reduce max_iterations or schema size

## Success Criteria
- [ ] Can learn from 5-10 example mods for a game in <5 minutes
- [ ] Can generate simple mod (value tweak) in <20 seconds with <3 tool calls
- [ ] Can generate medium mod (new item) in <60 seconds with 5-8 tool calls
- [ ] Can generate complex mod (new system) in <5 minutes with 10+ tool calls
- [ ] Generated mods pass syntax validation 95%+ of the time
- [ ] Generated mods pass reference checking 90%+ of the time
- [ ] Generated mods are installable and functional in actual games
- [ ] Agent successfully iterates and fixes validation errors autonomously
- [ ] Total cost for $250 budget supports 100+ complex mod generations
- [ ] Clear, actionable error messages when things go wrong
- [ ] Works reliably for all 6 initial games
- [ ] Agent demonstrates intelligent tool usage (researches before generating)
- [ ] Plans are appropriate to complexity (short for simple, detailed for complex)
- [ ] Users can understand agent's decision-making process via logs/verbose mode

### Recommended Development Order

**Phase 1-2: Foundation **
1. Set up project structure and dependencies
2. Implement OpenRouter client with basic tool support
3. Build schema learner for ONE game (start with CDDA - easiest JSON format)
4. Test learning from 5 example mods
5. Verify schema.json captures patterns correctly

**Phase 3: Tools Foundation **
1. Implement tool registry and executor
2. Build 3-5 core tools:
   - view_schema_file
   - list_examples  
   - validate_json
   - check_references
   - save_draft / load_draft
3. Test tool execution in isolation
4. Verify tool results format correctly for AI consumption

**Phase 4-5: Agent Core **
1. Build system prompt generator
2. Implement ModdingAgent orchestrator
3. Test with simple prompts ("add a sword")
4. Verify agentic loop (tool calls → results → more tool calls → completion)
5. Test self-assessed planning (does AI create appropriate plans?)
6. Verify iteration and refinement works

**Phase 6-7: Validation & First Game **
1. Implement all validators
2. Connect validators to agent workflow
3. Test end-to-end with CDDA:
   - Simple mod (spawn tweak)
   - Medium mod (new weapon)
   - Complex mod (new crafting category)
4. Refine based on results

**Phase 8: Expand & Polish **
1. Add remaining 5 games using same pattern
2. Implement CLI with all commands
3. Add logging, cost tracking, error handling
4. Write documentation and examples
5. Interactive mode
6. Batch processing
7. Mod refinement command

### Critical Early Decisions

**1. Tool Call Format**
OpenRouter supports OpenAI-style function calling:
```json
{
  "tools": [
    {
      "type": "function",
      "function": {
        "name": "view_schema_file",
        "description": "...",
        "parameters": {...}
      }
    }
  ]
}
```

Verify this works with Qwen3-Next-80B-A3B-Instruct early.

**2. Context Size Management**
Start with smaller context (8K) and scale up. Don't send full schema in system prompt - let agent fetch via tools.

**3. Iteration Limits**
Start with max_iterations=5 for testing, scale to 10 for production. Prevents runaway costs during development.

**4. Validation Strictness**
Start strict (all validation must pass). Relax later based on what breaks frequently.

### Testing Strategy

**Unit Tests:**
- Each tool in isolation
- Schema learning with known examples
- Validators with good/bad inputs
- Response parsing with various formats

**Integration Tests:**
- Agent workflow with mock OpenRouter
- Tool orchestration
- Validation feedback loop
- Cost tracking accuracy

**End-to-End Tests:**
- Real mod generation with actual API
- All 6 games with various complexities
- Failure modes (bad examples, missing docs)
- Cost limits and budget warnings

### Debugging Tips

**Agent not using tools:**
- Check tool definitions are in OpenAI format
- Verify system prompt encourages tool use
- Check if model supports function calling
- Add explicit "You MUST use tools" instructions

**Agent stuck in loop:**
- Check iteration counter is incrementing
- Verify termination conditions
- Look for tool calls returning same error repeatedly
- Add "If stuck, simplify scope" instruction

**High costs:**
- Check context size (trim old tool results)
- Verify not sending full schema unnecessarily  
- Look for redundant tool calls
- Add cost warnings earlier in workflow

**Validation always failing:**
- Check schema learning quality
- Verify validators aren't too strict
- Look at what AI is actually generating
- Add more examples if patterns unclear

## Extension Ideas (Beyond Core Functionality)

### Advanced Agent Features
- **Multi-agent architecture**: Specialist agents per game coordinate
- **Memory system**: Agent remembers user's preferred style across sessions
- **Learning from feedback**: Adjust schema based on what works/fails
- **Explanation mode**: Agent explains every decision in detail

### Tool Expansions
- **Version control tools**: Git integration for mod management
- **Dependency resolution**: Auto-find and include required mods
- **Compatibility checker**: Test against known mod conflicts
- **Performance profiler**: Estimate mod's game performance impact

### Integration Features  
- **Steam Workshop**: Auto-upload generated mods
- **Nexus Mods**: Integration with mod hosting
- **Discord bot**: Generate mods via Discord commands
- **Web UI**: Full-featured web interface for non-CLI users

### Quality Improvements
- **A/B testing**: Generate variants and let user pick best
- **User style learning**: Few-shot learning from user's mods
- **Automated testing**: Spin up game and test mod actually works
- **Community feedback loop**: Ratings improve schema quality

### Enterprise Features
- **Mod template library**: Save common patterns
- **Team collaboration**: Share schemas and tool configs
- **Batch pipelines**: Generate 100s of mods from CSV
- **Custom validation rules**: Per-project validation requirements
